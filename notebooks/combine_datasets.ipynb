{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_waves(s):\n",
    "    # Remove spaces\n",
    "    s = s.replace(\" \", \"\")\n",
    "    \n",
    "    # Split on commas\n",
    "    parts = s.split(\",\")\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for part in parts:\n",
    "        # Match either wN or wN-M\n",
    "        m = re.match(r\"w(\\d+)(?:-(\\d+))?\", part)\n",
    "        if m:\n",
    "            start = int(m.group(1))\n",
    "            end = int(m.group(2)) if m.group(2) else start\n",
    "            result.extend(range(start, end+1))\n",
    "    return result\n",
    "\n",
    "\n",
    "def expand_variable_rows(row):\n",
    "    waves_list = row[\"WavesList\"]\n",
    "    variable = row[\"Variable\"]\n",
    "    \n",
    "    if \"w\" in variable:\n",
    "        # variable has wave pattern\n",
    "        expanded = []\n",
    "        for w in waves_list:\n",
    "            # replace 'w' + digits with the wave number\n",
    "            new_variable = re.sub(r\"w\\d+\", f\"{w}\", variable)\n",
    "            expanded.append({\n",
    "                \"Topic\": row[\"Topic\"],\n",
    "                \"Variable\": new_variable,\n",
    "                \"Description\": row[\"Description\"],\n",
    "                \"Dataset\": row[\"Dataset\"],\n",
    "                \"Wave\": w\n",
    "            })\n",
    "        return expanded\n",
    "    else:\n",
    "        # keep as is, with Wave = None\n",
    "        return [{\n",
    "            \"Topic\": row[\"Topic\"],\n",
    "            \"Variable\": variable,\n",
    "            \"Description\": row[\"Description\"],\n",
    "            \"Dataset\": row[\"Dataset\"],\n",
    "            \"Wave\": None\n",
    "        }]\n",
    "    \n",
    "\n",
    "def rename_columns(df, country):\n",
    "    cols = df.columns.tolist()\n",
    "    new_cols = []\n",
    "    \n",
    "    if country == 'india':\n",
    "        # Replace any prefix ending with 1 â†’ 5\n",
    "        for c in cols:\n",
    "            m = re.match(r'^(s|r|h|hh)1(.*)', c)\n",
    "            if m:\n",
    "                new_c = f\"{m.group(1)}5{m.group(2)}\"\n",
    "                new_cols.append(new_c)\n",
    "            else:\n",
    "                new_cols.append(c)\n",
    "\n",
    "    elif country == 'mexico':\n",
    "        for c in cols:\n",
    "            m = re.match(r'^(s|r|h|hh)([1-4])(.*)', c)\n",
    "            if m:\n",
    "                old_wave = int(m.group(2))\n",
    "                new_wave = old_wave + 1\n",
    "                new_c = f\"{m.group(1)}{new_wave}{m.group(3)}\"\n",
    "                new_cols.append(new_c)\n",
    "            else:\n",
    "                new_cols.append(c)\n",
    "\n",
    "    elif country == 'uk':\n",
    "        wave_map = {\n",
    "            '8': '5',\n",
    "            '7': '4',\n",
    "            '6': '3',\n",
    "            '5': '2',\n",
    "            '4': '1'\n",
    "        }\n",
    "        for c in cols:\n",
    "            m = re.match(r'^(s|r|h|hh)([4-8])(.*)', c)\n",
    "            if m:\n",
    "                new_wave = wave_map[m.group(2)]\n",
    "                new_c = f\"{m.group(1)}{new_wave}{m.group(3)}\"\n",
    "                new_cols.append(new_c)\n",
    "            else:\n",
    "                new_cols.append(c)\n",
    "\n",
    "    elif country == 'us':\n",
    "        wave_map = {\n",
    "            '13': '5',\n",
    "            '12': '4',\n",
    "            '11': '3',\n",
    "            '10': '2',\n",
    "            '9': '1'\n",
    "        }\n",
    "        for c in cols:\n",
    "            m = re.match(r'^(s|r|h|hh)(13|12|11|10|9)(.*)', c)\n",
    "            if m:\n",
    "                new_wave = wave_map[m.group(2)]\n",
    "                new_c = f\"{m.group(1)}{new_wave}{m.group(3)}\"\n",
    "                new_cols.append(new_c)\n",
    "            else:\n",
    "                new_cols.append(c)\n",
    "\n",
    "    else:\n",
    "        # no renaming for other countries yet\n",
    "        new_cols = cols\n",
    "\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_columns(df, country, allowed_digits):\n",
    "    \"\"\"\n",
    "    Select columns in df that:\n",
    "    - either start with s/r/h/hh followed by one of the allowed digits\n",
    "    - or contain no digits at all\n",
    "    \"\"\"\n",
    "    digits_pattern = \"|\".join(sorted(allowed_digits[country], key=len, reverse=True))\n",
    "    # Build regex for allowed prefixes + digits\n",
    "    regex_pattern = re.compile(rf\"^(s|r|h|hh)({digits_pattern})\")\n",
    "\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if regex_pattern.match(col):\n",
    "            keep_cols.append(col)\n",
    "        elif not re.search(r\"\\d\", col):\n",
    "            keep_cols.append(col)\n",
    "    \n",
    "    return df[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_variables = pd.read_excel('../data/core/comparision/common_variables_short.xlsx')\n",
    "# table_variables['Topic'] = table_variables['Topic'].fillna(method=\"ffill\")\n",
    "# table_variables['Variable'] = table_variables['Variable'].fillna(method=\"ffill\")\n",
    "# table_variables['Description'] = table_variables['Description'].fillna(method=\"ffill\")\n",
    "# table_variables = table_variables[table_variables['Dataset'].notna()]\n",
    "# table_variables = table_variables[table_variables['Dataset'].apply(\n",
    "#     lambda x: ('HRS' in x) or ('MHAS' in x) or ('ELSA' in x) or ('LASI' in x))]\n",
    "# table_variables['Waves'] = table_variables['Dataset'].str.split(' - ').str[1]\n",
    "# table_variables['Dataset'] = table_variables['Dataset'].str.split(' - ').str[0]\n",
    "# table_variables['VariableNormalized'] = table_variables['Variable'].str.replace('Rw', '').str.lower()\n",
    "\n",
    "# table_variables.to_excel('../data/processed/table_variables.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_variables = pd.read_excel('../data/processed/table_variables.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Variable with AlternativeVariable if AlternativeVariable is not None\n",
    "table_variables['Variable'] = table_variables['AlternativeVariable'].where(table_variables['AlternativeVariable'].notnull(), table_variables['Variable'])\n",
    "table_variables = table_variables.drop(columns=['AlternativeVariable'])\n",
    "table_variables['WavesList'] = table_variables['Waves'].apply(lambda x: transform_waves(x))\n",
    "\n",
    "# Apply to each row and flatten the results\n",
    "expanded_rows = []\n",
    "for _, row in table_variables.iterrows():\n",
    "    expanded_rows.extend(expand_variable_rows(row))\n",
    "\n",
    "# Create new DataFrame\n",
    "table_variables_waves = pd.DataFrame(expanded_rows)\n",
    "\n",
    "mask = table_variables_waves[\"Wave\"].notnull()\n",
    "table_variables_waves.loc[mask, \"Variable\"] = table_variables_waves.loc[mask].apply(\n",
    "    lambda row: row[\"Variable\"].replace(\"w\", str(int(row[\"Wave\"]))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "table_variables_waves['Variable'] = table_variables_waves['Variable'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lasi = pd.read_stata('../data/core/LASI/Harmonized LASI A.3_Stata/H_LASI_a3.dta')\n",
    "df_lasi_dad = pd.read_stata('../data/hcap/LASI/Harmonized_LASI-DAD_Ver_B.1/H_LASI_DAD_b1.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lasi_dad = df_lasi_dad[['prim_key', 'r1sgcp', 'r2sgcp']]\n",
    "df_india = df_lasi.merge(df_lasi_dad, on='prim_key', how='inner')\n",
    "\n",
    "columns_india = table_variables_waves[table_variables_waves['Dataset'] == 'Harmonized LASI']['Variable'].tolist()\n",
    "# score_columns_india = ['r1sgcp', 'r2sgcp']\n",
    "# system_columns_india = ['prim_key', 'hhid', 'pnc', 'pn', 'h1coupid', 's1prim_key']\n",
    "score_columns_india = ['r1sgcp']\n",
    "system_columns_india = ['prim_key']\n",
    "\n",
    "selected_columns_india = list(set(df_india.columns) & set(columns_india))\n",
    "selected_columns_india = system_columns_india + selected_columns_india + score_columns_india\n",
    "\n",
    "df_india = df_india[selected_columns_india]\n",
    "\n",
    "# Convert all categorical columns to strings\n",
    "for col in df_india.columns:\n",
    "    if pd.api.types.is_categorical_dtype(df_india[col]):\n",
    "        df_india[col] = df_india[col].astype(str)\n",
    "\n",
    "df_india = df_india.rename(columns={'r1sgcp': 'fgcp', 'prim_key': 'id_resp'})\n",
    "df_india['country'] = 'india'\n",
    "\n",
    "df_india = df_india[df_india['fgcp'].notna()]\n",
    "\n",
    "df_india = df_india.rename(columns={'hh5rural': 'h5rural'})\n",
    "df_india = df_india.rename(columns={'r5coresd': 'h5coresd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india.to_parquet('../data/processed/df_india.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrs_harmonized = pd.read_spss('../data/core/HRS/H_HRS_d_spss/H_HRS_d.sav')\n",
    "df_hrs_rand = pd.read_stata('../data/core/HRS/randhrs1992_2022v1_STATA/randhrs1992_2022v1.dta')\n",
    "df_hrs_eol = pd.read_stata('../data/core/HRS/HarmonizedHRSEndOfLifeA/H_HRS_EOL_a.dta')\n",
    "df_hrs_scores = pd.read_stata(\n",
    "    '../data/hcap/HRS/HCAP-Harmonized-Factor-Scores/interim-HcapHarmoniz-305-v2-20230725-hrshcap.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrs_harmonized['hhidpn'] = df_hrs_harmonized['hhidpn'].astype(int)\n",
    "\n",
    "# Rename 'agey_b' to 'agey' in columns\n",
    "df_hrs_rand = df_hrs_rand.rename(columns=lambda x: x.replace('agey_b', 'agey'))\n",
    "\n",
    "df_hrs_scores = df_hrs_scores.drop(columns=['study'])\n",
    "df_hrs_scores = df_hrs_scores.rename(columns={'id_hrs': 'hhidpn'})\n",
    "\n",
    "df_hrs_harmonized['hhidpn'] = df_hrs_harmonized['hhidpn'].astype(int)\n",
    "df_hrs_rand['hhidpn'] = df_hrs_rand['hhidpn'].astype(int)\n",
    "df_hrs_eol['hhidpn'] = df_hrs_eol['hhidpn'].astype(int)\n",
    "df_hrs_scores['hhidpn'] = df_hrs_scores['hhidpn'].astype(int)\n",
    "\n",
    "df_us = df_hrs_harmonized.merge(df_hrs_rand, on=['hhidpn', 'hhid', 'pn'], how='left')\n",
    "df_us = df_us.merge(df_hrs_eol, on=['hhidpn', 'hhid', 'pn'], how='left')\n",
    "df_us = df_us.merge(df_hrs_scores, on='hhidpn', how='inner')\n",
    "\n",
    "datasets_us = set(['Harmonized HRS', 'RAND HRS Longitudinal File', 'Harmonized HRS EOL'])\n",
    "columns_us = table_variables_waves[table_variables_waves['Dataset'].isin(datasets_us)]['Variable'].tolist()\n",
    "score_columns_us = ['fgcp']\n",
    "system_columns_us = ['hhidpn']\n",
    "\n",
    "selected_columns_us = list(set(df_us.columns) & set(columns_us))\n",
    "selected_columns_us = system_columns_us + selected_columns_us + score_columns_us\n",
    "df_us = df_us[selected_columns_us]\n",
    "\n",
    "# Convert all categorical columns to strings\n",
    "for col in df_us.columns:\n",
    "    if pd.api.types.is_categorical_dtype(df_us[col]):\n",
    "        df_us[col] = df_us[col].astype(str)\n",
    "\n",
    "df_us = df_us.rename(columns={'hhidpn': 'id_resp'})\n",
    "df_us['country'] = 'us'\n",
    "\n",
    "# Rename columns\n",
    "df_us = df_us.rename(columns={\n",
    "    'r1doctor': 'r1doctor1y',\n",
    "    'r2doctor': 'r2doctor1y',\n",
    "    'r3doctor': 'r3doctor1y',\n",
    "    'r4doctor': 'r4doctor1y',\n",
    "    'r5doctor': 'r5doctor1y'\n",
    "})\n",
    "\n",
    "df_us = df_us.rename(columns={\n",
    "    'r1sayret': 'r1retemp',\n",
    "    'r2sayret': 'r2retemp',\n",
    "    'r3sayret': 'r3retemp',\n",
    "    'r4sayret': 'r4retemp',\n",
    "    'r5sayret': 'r5retemp'\n",
    "})\n",
    "\n",
    "df_us = df_us.rename(columns={\n",
    "    'rassrecv': 'r1pubpen'\n",
    "})\n",
    "\n",
    "df_us['r2pubpen'] = df_us['r1pubpen']\n",
    "df_us['r3pubpen'] = df_us['r1pubpen']\n",
    "df_us['r4pubpen'] = df_us['r1pubpen']\n",
    "df_us['r5pubpen'] = df_us['r1pubpen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.to_parquet('../data/processed/df_us.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elsa = pd.read_stata('../data/core/ELSA/UKDA-5050-stata/stata/stata13_se/h_elsa_g3.dta')\n",
    "df_elsa_scores = pd.read_spss('../data/hcap/ELSA/spss/spss28/hcap_2018_harmonised_scores.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elsa_scores['id_elsa'] = df_elsa_scores['id_elsa'].astype(int)\n",
    "df_elsa_scores = df_elsa_scores.drop(columns=['study'])\n",
    "df_uk = df_elsa.merge(df_elsa_scores, left_on='idauniq', right_on='id_elsa', how='inner')\n",
    "\n",
    "columns_uk = table_variables_waves[table_variables_waves['Dataset'] == 'Harmonized ELSA']['Variable'].tolist()\n",
    "score_columns_uk = ['fgcp']\n",
    "system_columns_uk = ['id_elsa']\n",
    "\n",
    "selected_columns_uk = list(set(df_elsa.columns) & set(columns_uk))\n",
    "selected_columns_uk = system_columns_uk + selected_columns_uk + score_columns_uk\n",
    "\n",
    "df_uk = df_uk[selected_columns_uk]\n",
    "\n",
    "# Convert all categorical columns to strings\n",
    "for col in df_uk.columns:\n",
    "    if pd.api.types.is_categorical_dtype(df_uk[col]):\n",
    "        df_uk[col] = df_uk[col].astype(str)\n",
    "\n",
    "df_uk = df_uk.rename(columns={'id_elsa': 'id_resp'})\n",
    "df_uk['country'] = 'uk'\n",
    "\n",
    "# Remove '_e' from columns\n",
    "df_uk = df_uk.rename(columns=lambda x: x.replace('_e', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.to_parquet('../data/processed/df_uk.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mhas = pd.read_spss('../data/core/MHAS/H_MHAS_c2.sav')\n",
    "df_mexcog = pd.read_stata('../data/hcap/Mex-Cog/Multi-Country Harmonized Factor Scores Mex-Cog 2016.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mhas['cunicah'] = df_mhas['cunicah'].astype(int)\n",
    "df_mhas['np'] = df_mhas['np'].astype(int)\n",
    "df_mexcog['cunicah'] = df_mexcog['cunicah'].astype(int)\n",
    "df_mexcog['np'] = df_mexcog['np'].astype(int)\n",
    "\n",
    "df_mexico = df_mhas.merge(df_mexcog, on=['cunicah', 'np'], how='inner')\n",
    "\n",
    "columns_mexico = table_variables_waves[table_variables_waves['Dataset'] == 'Harmonized MHAS']['Variable'].tolist()\n",
    "score_columns_mexico = ['fgcp']\n",
    "system_columns_mexico = ['unhhidnp']\n",
    "\n",
    "selected_columns_mexico = list(set(df_mexico.columns) & set(columns_mexico))\n",
    "selected_columns_mexico = system_columns_mexico + selected_columns_mexico + score_columns_mexico\n",
    "\n",
    "df_mexico = df_mexico[selected_columns_mexico]\n",
    "\n",
    "# Convert all categorical columns to strings\n",
    "for col in df_mexico.columns:\n",
    "    if pd.api.types.is_categorical_dtype(df_mexico[col]):\n",
    "        df_mexico[col] = df_mexico[col].astype(str)\n",
    "\n",
    "df_mexico = df_mexico.rename(columns={'unhhidnp': 'id_resp'})\n",
    "df_mexico['country'] = 'mexico'\n",
    "\n",
    "# Remove '_m' from columns\n",
    "df_mexico = df_mexico.rename(columns=lambda x: x.replace('_m', ''))\n",
    "\n",
    "df_mexico.to_parquet('../data/processed/df_mexico.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used waves\n",
    "\n",
    "### 1. India\n",
    "\n",
    "- X waves: 1 (2017â€“2019)\n",
    "- Y: 2017-2019\n",
    "\n",
    "### 2. US\n",
    "\n",
    "- X waves:\n",
    "\n",
    "    9 (2008â€“2009)\n",
    "\n",
    "    10 (2010â€“2011)\n",
    "\n",
    "    11 (2012â€“2013)\n",
    "\n",
    "    12 (2014â€“2015)\n",
    "\n",
    "    13 (2016â€“2018)\n",
    "\n",
    "- Y: 2016\n",
    "\n",
    "### 3. UK\n",
    "\n",
    "- X waves:\n",
    "\n",
    "    4 May 2008 â€“ Jul 2009\n",
    "\n",
    "    5 Jun 2010 â€“ Jul 2011\n",
    "\n",
    "    6 May 2012 â€“ Jun 2013\n",
    "\n",
    "    7 Jun 2014 â€“ May 2015\n",
    "\n",
    "    8 May 2016 â€“ Jun 2017\n",
    "\n",
    "- Y: 15 January 2018 and 8 April 2018\n",
    "\n",
    "### 4. Mexico\n",
    "\n",
    "- X waves:\n",
    "\n",
    "\t1 - 2001\n",
    "\n",
    "\t2 - 2003\n",
    "\n",
    "\t3 - 2012\n",
    "\n",
    "\t4 - 2015\n",
    "\n",
    "- Y: 2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_india = pd.read_parquet('../data/processed/df_india.parquet')\n",
    "df_us = pd.read_parquet('../data/processed/df_us.parquet')\n",
    "df_uk = pd.read_parquet('../data/processed/df_uk.parquet')\n",
    "df_mexico = pd.read_parquet('../data/processed/df_mexico.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/l43pc0gj6311hvl_9yxc3kzc0000gn/T/ipykernel_28863/127253590.py:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_combined = pd.concat([df_us, df_uk, df_mexico, df_india])\n"
     ]
    }
   ],
   "source": [
    "df_india['id_resp'] = df_india['country'] + '_' + df_india['id_resp'].astype(int).astype(str)\n",
    "df_us['id_resp'] = df_us['country'] + '_' + df_us['id_resp'].astype(int).astype(str)\n",
    "df_uk['id_resp'] = df_uk['country'] + '_' + df_uk['id_resp'].astype(int).astype(str)\n",
    "df_mexico['id_resp'] = df_mexico['country'] + '_' + df_mexico['id_resp'].astype(int).astype(str)\n",
    "\n",
    "# Map of allowed digits for each country\n",
    "allowed_waves = {\n",
    "    \"us\": ['9', '10', '11', '12', '13'],\n",
    "    \"uk\": ['4', '5', '6', '7', '8'],\n",
    "    \"mexico\": ['1', '2', '3', '4'],\n",
    "    \"india\": ['1']\n",
    "}\n",
    "\n",
    "df_india = select_columns(df_india, 'india', allowed_waves)\n",
    "df_us = select_columns(df_us, 'us', allowed_waves)\n",
    "df_uk = select_columns(df_uk, 'uk', allowed_waves)\n",
    "df_mexico = select_columns(df_mexico, 'mexico', allowed_waves)\n",
    "\n",
    "df_us = rename_columns(df_us, 'us')\n",
    "df_uk = rename_columns(df_uk, 'uk')\n",
    "df_mexico = rename_columns(df_mexico, 'mexico')\n",
    "df_india = rename_columns(df_india, 'india')\n",
    "\n",
    "df_mexico = df_mexico.drop(columns=['raedyrs'])\n",
    "df_us = df_us.drop(columns=['raedyrs'])\n",
    "df_india = df_india.drop(columns=['raedyrs'])\n",
    "df_uk = df_uk.drop(columns=['raedyrs'])\n",
    "\n",
    "all_columns = sorted(set(df_us.columns) | set(df_uk.columns) | set(df_mexico.columns) | set(df_india.columns))\n",
    "\n",
    "df_us = df_us.reindex(columns=all_columns)\n",
    "df_uk = df_uk.reindex(columns=all_columns)\n",
    "df_mexico = df_mexico.reindex(columns=all_columns)\n",
    "df_india = df_india.reindex(columns=all_columns)\n",
    "\n",
    "df_combined = pd.concat([df_us, df_uk, df_mexico, df_india])\n",
    "\n",
    "df_combined[\"id_resp\"] = df_combined[\"id_resp\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_notna_countries(df_combined):\n",
    "\n",
    "#     # Compute % missing for each country\n",
    "#     result = pd.DataFrame()\n",
    "\n",
    "#     for name, group in df_combined.groupby(\"country\"):\n",
    "#         pct_missing = 100 * group.notna().sum() / len(group)\n",
    "#         pct_missing = pct_missing.drop(\"country\", errors=\"ignore\")\n",
    "#         result[name] = pct_missing\n",
    "\n",
    "#     # Optional formatting\n",
    "#     result = result.round(1)\n",
    "\n",
    "#     # Put variables as a column instead of index if desired\n",
    "#     result = result.reset_index()\n",
    "#     result = result.rename(columns={'index': 'variable'})\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# notna_countries = compute_notna_countries(df_combined)\n",
    "# notna_countries = notna_countries.set_index('variable')\n",
    "# notna_countries = notna_countries > 0\n",
    "\n",
    "# notna_countries['sum'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.set_index('id_resp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('../data/processed/df_combined.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"r1doctor\",\n",
    "    \"r1sayret\",\n",
    "    \"r2doctor\",\n",
    "    \"r2sayret\",\n",
    "    \"r3doctor\",\n",
    "    \"r3sayret\",\n",
    "    \"r4doctor\",\n",
    "    \"r4sayret\",\n",
    "    \"r5coresd\",\n",
    "    \"r5doctor\",\n",
    "    \"r5sayret\",\n",
    "    'hh5rural'\n",
    "]\n",
    "\n",
    "df_combined = df_combined.drop(columns=cols_to_drop, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "df_combined.to_pickle('../data/processed/df_combined.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social_determinants_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
