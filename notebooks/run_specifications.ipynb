{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da9668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "# Add imports for advanced statistics\n",
    "from scipy.stats import entropy, hmean, gmean\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea624527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prepare-submission/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')  # Add project root to Python path\n",
    "from src.pipeline import PipelineRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa330bbf",
   "metadata": {},
   "source": [
    "# 1. Feature specifications\n",
    "\n",
    "1. Only basic features\n",
    "\n",
    "2. Add logs of numerical features\n",
    "\n",
    "3. Add group level aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('../data/processed/df_combined.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b00929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic features\n",
    "runner = PipelineRunner(data_dir='../data', models_dir='../models', run_name='basic',\n",
    "                        df_combined_path='../data/processed/df_combined.pkl',\n",
    "                        variables_type_path='../data/variables/table_variables_type.xlsx',\n",
    "                        add_logs=False,\n",
    "                        add_group_aggregations=False,\n",
    "                        return_shap=True,\n",
    "                        return_ci=True)\n",
    "\n",
    "_ = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Add logs features\n",
    "runner = PipelineRunner(data_dir='../data', models_dir='../models', run_name='logs',\n",
    "                        df_combined_path='../data/processed/df_combined.pkl',\n",
    "                        variables_type_path='../data/variables/table_variables_type.xlsx',\n",
    "                        add_logs=True,\n",
    "                        add_group_aggregations=False)\n",
    "\n",
    "_ = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e01f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add group level aggregations\n",
    "runner = PipelineRunner(data_dir='../data', models_dir='../models', run_name='group_agg',\n",
    "                        df_combined_path='../data/processed/df_combined.pkl',\n",
    "                        variables_type_path='../data/variables/table_variables_type.xlsx',\n",
    "                        add_logs=True,\n",
    "                        add_group_aggregations=True)\n",
    "\n",
    "_ = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cd407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ad5672",
   "metadata": {},
   "source": [
    "# 2. Different Cross-validation\n",
    "\n",
    "1. Random split\n",
    "\n",
    "2. Leave-one-country-out\n",
    "\n",
    "3. Leave-one-country-out with partial holdout\n",
    "\n",
    "4. Only one country (change dataset) with random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a78c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the countries and holdout modes\n",
    "list_countries = ['mexico', 'us', 'uk', 'india']\n",
    "holdout_modes = ['partial_holdout', 'full_holdout']\n",
    "\n",
    "# Run all combinations\n",
    "for country in list_countries:\n",
    "    for holdout_mode in holdout_modes:\n",
    "        run_name = f'{country}_{holdout_mode}'\n",
    "        \n",
    "        print(f\"\\n=== Running: {run_name} ===\")\n",
    "        \n",
    "        # Create and run pipeline\n",
    "        runner = PipelineRunner(\n",
    "            data_dir='../data',\n",
    "            models_dir='../models',\n",
    "            run_name=run_name,\n",
    "            df_combined_path='../data/processed/df_combined.pkl',\n",
    "            variables_type_path='../data/variables/table_variables_type.xlsx',\n",
    "            country_holdout=country,\n",
    "            holdout_split_mode=holdout_mode\n",
    "        )\n",
    "        \n",
    "        predictions = runner.run_full_pipeline()\n",
    "        print(f\"âœ… Completed: {run_name}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All pipelines completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate datasets for each country\n",
    "df_combined = pd.read_pickle('../data/processed/df_combined.pkl')\n",
    "\n",
    "for country in df_combined['country'].unique():\n",
    "    df_country = df_combined[df_combined['country'] == country]\n",
    "    df_country.to_pickle(f'../data/processed/df_combined_{country}.pkl')\n",
    "    print(f'Saved {country} dataset shape: {df_country.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a58bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run country-specific pipeline\n",
    "for country in ['mexico', 'us', 'uk', 'india']:\n",
    "    print(f' === Running {country} pipeline ===')\n",
    "    runner = PipelineRunner(data_dir='../data', models_dir='../models', run_name=f'{country}',\n",
    "                            df_combined_path=f'../data/processed/df_combined_{country}.pkl',\n",
    "                            variables_type_path='../data/variables/table_variables_type.xlsx')\n",
    "    _ = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790ba98",
   "metadata": {},
   "source": [
    "# 3. Robustness checks\n",
    "\n",
    "1. Remove random 10%, 30%, 50% of features\n",
    "2. Remove random 10%, 30%, 50% of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('../data/processed/df_combined.pkl')\n",
    "keep_cols = ['country', 'fgcp', 'age_group'\n",
    "             'r1agey', 'r2agey', 'r3agey', 'r4agey', 'r5agey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baa149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features that can be removed (all columns except keep_cols)\n",
    "removable_features = [col for col in df_combined.columns if col not in keep_cols]\n",
    "print(f\"Total features: {len(df_combined.columns)}\")\n",
    "print(f\"Removable features: {len(removable_features)}\")\n",
    "\n",
    "# Define removal percentages\n",
    "removal_percentages = [10, 30, 50, 70, 90]\n",
    "\n",
    "# Create datasets with different feature removal percentages\n",
    "for pct in removal_percentages:\n",
    "    print(f\"\\nCreating dataset with {pct}% features removed...\")\n",
    "    \n",
    "    # Calculate number of features to remove\n",
    "    n_features_to_remove = int(len(removable_features) * pct / 100)\n",
    "    \n",
    "    # Randomly select features to remove\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    features_to_remove = np.random.choice(removable_features, size=n_features_to_remove, replace=False)\n",
    "    \n",
    "    # Create new dataset by dropping selected features\n",
    "    df_reduced = df_combined.drop(columns=features_to_remove)\n",
    "    \n",
    "    # Save the reduced dataset\n",
    "    filename = f'../data/processed/df_combined_remove_{pct}pct_features.pkl'\n",
    "    df_reduced.to_pickle(filename)\n",
    "    \n",
    "    print(f\"  Removed {n_features_to_remove} features\")\n",
    "    print(f\"  New dataset shape: {df_reduced.shape}\")\n",
    "    print(f\"  Saved as: df_combined_remove_{pct}pct_features.pkl\")\n",
    "\n",
    "print(f\"\\nâœ… All feature-reduced datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe161bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original dataset shape: {df_combined.shape}\")\n",
    "\n",
    "# Define removal percentages\n",
    "removal_percentages = [10, 30, 50, 70, 90]\n",
    "\n",
    "# Create datasets with different observation removal percentages\n",
    "for pct in removal_percentages:\n",
    "    print(f\"\\nCreating dataset with {pct}% observations removed...\")\n",
    "    \n",
    "    # Calculate number of observations to keep (remove pct%)\n",
    "    n_obs_to_keep = int(len(df_combined) * (100 - pct) / 100)\n",
    "    \n",
    "    # Randomly select observations to keep\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    keep_indices = np.random.choice(df_combined.index, size=n_obs_to_keep, replace=False)\n",
    "    \n",
    "    # Create new dataset with selected observations\n",
    "    df_reduced = df_combined.loc[keep_indices].copy()\n",
    "    \n",
    "    # Save the reduced dataset\n",
    "    filename = f'../data/processed/df_combined_remove_{pct}pct_obs.pkl'\n",
    "    df_reduced.to_pickle(filename)\n",
    "    \n",
    "    print(f\"  Kept {n_obs_to_keep} observations (removed {len(df_combined) - n_obs_to_keep})\")\n",
    "    print(f\"  New dataset shape: {df_reduced.shape}\")\n",
    "    print(f\"  Saved as: df_combined_remove_{pct}pct_obs.pkl\")\n",
    "\n",
    "print(f\"\\nâœ… All observation-reduced datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Running pipelines for feature-reduced datasets ===\")\n",
    "\n",
    "removal_percentages = [10, 30, 50, 70, 90]\n",
    "\n",
    "for pct in removal_percentages:\n",
    "    run_name = f'reduced_features_{pct}pct'\n",
    "    dataset_path = f'../data/processed/df_combined_remove_{pct}pct_features.pkl'\n",
    "    \n",
    "    print(f\"\\n--- Running: {run_name} ---\")\n",
    "    \n",
    "    runner = PipelineRunner(\n",
    "        data_dir='../data',\n",
    "        models_dir='../models',\n",
    "        run_name=run_name,\n",
    "        df_combined_path=dataset_path,\n",
    "        variables_type_path='../data/variables/table_variables_type.xlsx'\n",
    "    )\n",
    "    \n",
    "    predictions = runner.run_full_pipeline()\n",
    "    print(f\"âœ… Completed: {run_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Running pipelines for observation-reduced datasets ===\")\n",
    "\n",
    "for pct in removal_percentages:\n",
    "    run_name = f'reduced_obs_{pct}pct'\n",
    "    dataset_path = f'../data/processed/df_combined_remove_{pct}pct_obs.pkl'\n",
    "    \n",
    "    print(f\"\\n--- Running: {run_name} ---\")\n",
    "    \n",
    "    runner = PipelineRunner(\n",
    "        data_dir='../data',\n",
    "        models_dir='../models',\n",
    "        run_name=run_name,\n",
    "        df_combined_path=dataset_path,\n",
    "        variables_type_path='../data/variables/table_variables_type.xlsx'\n",
    "    )\n",
    "    \n",
    "    predictions = runner.run_full_pipeline()\n",
    "    print(f\"âœ… Completed: {run_name}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All robustness check pipelines completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9755f9",
   "metadata": {},
   "source": [
    "# 4. Remove waves\n",
    "\n",
    "1. Dataset without India as baseline\n",
    "1. Dataset without wave 5\n",
    "2. Dataset without wave 4\n",
    "3. Dataset without wave 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03adf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_pickle('../data/processed/df_combined.pkl')\n",
    "df_combined_no_india = df_combined[df_combined['country'] != 'india']\n",
    "df_combined_no_india.to_pickle('../data/processed/df_combined_no_india.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PipelineRunner(\n",
    "    data_dir='../data',\n",
    "    models_dir='../models',\n",
    "    run_name='no_india',\n",
    "    df_combined_path='../data/processed/df_combined_no_india.pkl',\n",
    "    variables_type_path='../data/variables/table_variables_type.xlsx'\n",
    ") \n",
    "predictions = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wave patterns to remove\n",
    "wave_patterns = {\n",
    "    'no_wave5': ['r5', 'h5'],\n",
    "    'no_wave45': ['r5', 'h5', 'r4', 'h4'], \n",
    "    'no_wave345': ['r5', 'h5', 'r4', 'h4', 'r3', 'h3']\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "for name, patterns in wave_patterns.items():\n",
    "    # Find columns to remove\n",
    "    cols_to_remove = [col for col in df_combined.columns \n",
    "                     if any(col.startswith(p) for p in patterns) \n",
    "                     and col not in keep_cols]\n",
    "    \n",
    "    # Create and save dataset\n",
    "    df_reduced = df_combined_no_india.drop(columns=cols_to_remove)\n",
    "    filename = f'../data/processed/df_combined_{name}.pkl'\n",
    "    df_reduced.to_pickle(filename)\n",
    "    \n",
    "    print(f\"{name}: {df_reduced.shape} (removed {len(cols_to_remove)} columns)\")\n",
    "\n",
    "print(\"âœ… All wave-reduced datasets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a416e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipelines for wave-reduced datasets\n",
    "wave_datasets = ['no_wave5', 'no_wave45', 'no_wave345']\n",
    "\n",
    "for dataset_name in wave_datasets:\n",
    "    run_name = dataset_name\n",
    "    dataset_path = f'../data/processed/df_combined_{dataset_name}.pkl'\n",
    "    \n",
    "    print(f\"\\n=== Running: {run_name} ===\")\n",
    "    \n",
    "    runner = PipelineRunner(\n",
    "        data_dir='../data',\n",
    "        models_dir='../models',\n",
    "        run_name=run_name,\n",
    "        df_combined_path=dataset_path,\n",
    "        variables_type_path='../data/variables/table_variables_type.xlsx'\n",
    "    )\n",
    "    \n",
    "    predictions = runner.run_full_pipeline()\n",
    "    print(f\"âœ… Completed: {run_name}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All wave-reduced pipelines completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3e0d4",
   "metadata": {},
   "source": [
    "# 5. Bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['country', 'ragender_2_0', 'raeducl']:\n",
    "\n",
    "    runner = PipelineRunner(data_dir='../data', models_dir='../models', run_name=f'fair_{col}',\n",
    "                            df_combined_path='../data/processed/df_combined.pkl',\n",
    "                            variables_type_path='../data/variables/table_variables_type.xlsx',\n",
    "                            fair_col=col)\n",
    "\n",
    "    _ = runner.run_full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913c12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prepare-submission",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
